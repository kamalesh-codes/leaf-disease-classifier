{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QDosQr-E9wcw",
    "outputId": "ef02da65-5345-4c52-b11d-6188634bf93d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from torchvision import transforms,models\n",
    "from torchmetrics import Accuracy,Precision,Recall,CohenKappa,F1Score\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "from torchmetrics.utilities.plot import plot_confusion_matrix\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmKcsLr9tfdm",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs':100,\n",
    "                  'lr':0.005,\n",
    "                  'patience':20,\n",
    "                  'train_size':0.8,\n",
    "                  'val_size':0.1,\n",
    "                  'test_size':0.1,\n",
    "                  'train_batch_size':32,\n",
    "                  'val_batch_size':32,\n",
    "                  'test_batch_size':32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "973jazynwdrw",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self,path,transform=None):\n",
    "    f = np.load(path)\n",
    "    self.image = f[\"image\"].astype(np.uint8)\n",
    "    self.label = torch.tensor(f[\"label\"],dtype = torch.long)\n",
    "    self.transform =  transform\n",
    "  def __len__(self):\n",
    "    return self.label.shape[0]\n",
    "\n",
    "  def __getitem__(self,index):\n",
    "    if self.transform:\n",
    "      return self.transform(self.image[index]),self.label[index]\n",
    "    return self.image[index],self.label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "geDMZAR_y2pb",
    "outputId": "5d2185a5-d3f1-4df5-8d43-1fb3eb1c7181",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAG-vcfMh4GX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class_encode = {\"healthy\":0,\"angular_leafspot\":1,\n",
    "                \"Calciumdeficiency\":2,\"Leaf_scorch\":3,\"leaf_spot\":4}\n",
    "class_decode = {0:\"healthy\",1:\"angular_leafspot\",\n",
    "                2:\"Calciumdeficiency\",3:\"Leaf_scorch\",4:\"leaf_spot\"}\n",
    "num_class = 5\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                std=[75.640,92.417, 91.233, ],\n",
    "                                mean=[  20.912,2.458, -22.453]),# In (batch,C,H,W) format\n",
    "\n",
    "])\n",
    "dataset = CustomDataset(path=dataset_path,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RagfintypoV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_split,val_split,test_split = random_split(dataset,[hyperparameters['train_size'],\n",
    "                                                         hyperparameters['val_size'],\n",
    "                                                         hyperparameters['test_size']])\n",
    "\n",
    "train_loader = DataLoader(train_split,batch_size = hyperparameters['train_batch_size'],shuffle=True,num_workers =4)\n",
    "val_loader = DataLoader(val_split,batch_size = hyperparameters['val_batch_size'],shuffle=False)\n",
    "test_loader = DataLoader(test_split,batch_size = hyperparameters['test_batch_size'],shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4yHhWvLGuXL9",
    "outputId": "6651524f-4268-4545-f1b8-21663ad414e4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#dataset class count check\n",
    "class_count = {k:0 for k in class_encode.keys()}\n",
    "for _,label in dataset:\n",
    "  class_count[class_decode[label.item()]]+=1\n",
    "print(f'{class_count} total={sum(class_count.values())}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Yxx3PE7-AUG-",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BCdEmRVSGTh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,num_class,device,path_to_pretrained_weight=None):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet34(weights=None).to(torch.float32)\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "                        nn.Linear(in_features=self.resnet.fc.in_features,out_features=1024),\n",
    "                        nn.Dropout(p=0.5),\n",
    "                        nn.Linear(in_features=1024,out_features=num_class)\n",
    "        )\n",
    "        if path_to_pretrained_weight:\n",
    "            pretrained_weight = torch.load(path_to_pretrained_weight,map_location=torch.device(device))\n",
    "            model_state_dict = self.resnet.state_dict()\n",
    "            model_state_dict.update({layer_name:layer for layer_name,layer in pretrained_weight.items() if \"fc\" not in layer_name})\n",
    "            self.resnet.load_state_dict(model_state_dict)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NV_wdUvJ9TYW",
    "outputId": "de374603-ac2f-4dae-e149-c64528b6449c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Model(num_class=num_class,path_to_pretrained_weight=pretrained_model_path,device=device)\n",
    "model.to(device)\n",
    "#freeze all layers except layer4 and fc\n",
    "for name,layer in model.named_parameters():\n",
    "    if 'layer4' in name or 'fc' in name:\n",
    "        layer.requires_grad = True\n",
    "        # print(f\"{name} {layer.requires_grad}\")\n",
    "    else:\n",
    "        layer.requires_grad = False\n",
    "        # print(f\"{name} {layer.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XrCU7LfE-KY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),lr = hyperparameters['lr'],weight_decay = 0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yHYTcVdkPj-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_metrics = MetricCollection({\"accuracy\":Accuracy(task=\"multiclass\",num_classes=num_class),\n",
    "                           \"precision\":Precision(task=\"multiclass\",average=\"macro\",num_classes=num_class),\n",
    "                           \"recall\":Recall(task=\"multiclass\",average=\"macro\",num_classes=num_class),\n",
    "                           \"f1score\":F1Score(task=\"multiclass\",average=\"macro\",num_classes=num_class),\n",
    "                           \"kappa\":CohenKappa(task=\"multiclass\",num_classes=num_class)\n",
    "})\n",
    "confmat = MulticlassConfusionMatrix(num_classes=num_class).to(device)\n",
    "\n",
    "train_metrics = base_metrics.clone(prefix='train_').to(device)\n",
    "val_metrics = base_metrics.clone(prefix='val_').to(device)\n",
    "test_metrics = base_metrics.clone(prefix='test_').to(device)\n",
    "\n",
    "metric_names = (\n",
    "\"accuracy\",\n",
    "\"precision\",\n",
    "\"recall\",\n",
    "\"f1score\",\n",
    "\"kappa\",\n",
    "\"epochs_loss_values\",\n",
    "\"batch_loss\")\n",
    "\n",
    "log_batch = 100 #Compute and log the metrics values after every log_batch batches\n",
    "\n",
    "log_data={}\n",
    "for data_name in (\"training\",\"validation\",'testing'):\n",
    "    log_data[data_name]  = dict()\n",
    "    for prefix in ('train','test','val'):\n",
    "        if prefix not in data_name:\n",
    "            continue\n",
    "        for name in metric_names:\n",
    "            log_data[data_name][prefix+\"_\"+name]  = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqhbuD9iSGTk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def save_model(path,model_dict,model_name=\"model\"):\n",
    "    now=datetime.now()\n",
    "    full_path = f'{path}/{model_name}@{now}.pth'\n",
    "    torch.save(model_dict,full_path)\n",
    "    print(\"Model is saved\")\n",
    "    return full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jp4jwpudXp4J",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "min_epoch_loss = float('inf')\n",
    "curr_limit = 0\n",
    "best_epoch = 0\n",
    "best_model_path = None\n",
    "train_metrics.reset()\n",
    "val_metrics.reset()\n",
    "for epoch in range(hyperparameters['epochs']):\n",
    "  train_epoch_loss = 0.0\n",
    "  val_epoch_loss = 0.0\n",
    "\n",
    "  for batch_num,(train_image,train_label) in enumerate(train_loader):\n",
    "    model.train()\n",
    "    train_image = train_image.to(device)\n",
    "    train_label = train_label.to(device).squeeze()\n",
    "\n",
    "    train_output = model(train_image)#forward propagation\n",
    "    train_loss = criterion(train_output,train_label)#calculate loss\n",
    "    optimizer.zero_grad()#zero the gradients\n",
    "    train_loss.backward()#backpropagate the loss\n",
    "    optimizer.step()#update the parameters\n",
    "\n",
    "    #logging the metric values\n",
    "    train_epoch_loss+=train_loss.item()\n",
    "    log_data['training']['train_batch_loss'].append(train_loss.item())\n",
    "\n",
    "    #adding taining metrics\n",
    "    train_metrics.update(train_output,train_label)\n",
    "    if (batch_num+1)%log_batch==0:\n",
    "      computed_train_metrics = train_metrics.compute()\n",
    "      for metric,value in computed_train_metrics.items():\n",
    "        log_data['training'][metric].append(value.item())\n",
    "      train_metrics.reset()\n",
    "\n",
    "    #validating on validation data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for val_image,val_label in val_loader:\n",
    "        val_image = val_image.to(device)\n",
    "        val_label = val_label.to(device).squeeze()\n",
    "        val_output = model(val_image)\n",
    "        val_loss = criterion(val_output,val_label)\n",
    "        val_epoch_loss+=val_loss.item()\n",
    "        log_data['validation'][\"val_batch_loss\"].append(val_loss.item())\n",
    "\n",
    "        #adding validation metrics\n",
    "        val_metrics.update(val_output,val_label)\n",
    "      #computing for overall batches\n",
    "      if (batch_num+1)%log_batch==0:\n",
    "        computed_val_metrics = val_metrics.compute()\n",
    "        for metric,value in computed_val_metrics.items():\n",
    "          log_data['validation'][metric].append(value.item())\n",
    "        val_metrics.reset()\n",
    "\n",
    "    if (batch_num+1)%log_batch==0:\n",
    "      print(f\"epoch {epoch} batch {batch_num} | train loss: {train_loss.item():.3f} accu {computed_train_metrics['train_accuracy'].item():.3f} | val loss: {val_loss.item():.3f} accu {computed_val_metrics['val_accuracy'].item():.3f}\")\n",
    "    else:\n",
    "      print(f\"epoch {epoch} batch {batch_num} | train loss: {train_loss.item():.3f}| val loss: {val_loss.item():.3f}\")\n",
    "\n",
    "  log_data['training'][\"train_epochs_loss_values\"].append(train_epoch_loss)\n",
    "  log_data['validation'][\"val_epochs_loss_values\"].append(val_epoch_loss)\n",
    "\n",
    "  print(f'At epoch {epoch} train loss is {train_epoch_loss}')\n",
    "  print(f'At epoch {epoch} valuation loss is {val_epoch_loss}')\n",
    "\n",
    "\n",
    "  if val_epoch_loss<min_epoch_loss:\n",
    "    min_epoch_loss = val_epoch_loss\n",
    "    curr_limit = 0\n",
    "    best_epoch = epoch\n",
    "    best_model_path = save_model(path=model_saving_path,model_dict=model.state_dict(),model_name=f'epoch:{epoch}')\n",
    "  else:\n",
    "    curr_limit+=1\n",
    "    if curr_limit>=hyperparameters['patience']:\n",
    "      print(\"Early stopping is trigered!\")\n",
    "      print(f\"last model saved is in epoch {best_epoch}\")\n",
    "      break\n",
    "\n",
    "print('finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8qwmVwTankq",
    "outputId": "1fb3e30b-527b-4838-ba18-4f50613a4731",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#load the best model\n",
    "state_dict_best_model = torch.load(best_model_path,map_location=device)\n",
    "model.load_state_dict(state_dict_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "id": "GzqHRSPfbyLv",
    "outputId": "7d1d85aa-51b5-4987-9b53-cd465e51e65c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_axis(*args):\n",
    "    data,ax,label,color,x_axis,y_axis = args\n",
    "    ax.plot(data,label=label,color=color)\n",
    "    ax.set_xlabel(x_axis)\n",
    "    ax.set_ylabel(y_axis)\n",
    "    ax.legend()\n",
    "\n",
    "fig,axes = plt.subplots(4,2,figsize=(15,10))\n",
    "display_axis(log_data['training']['train_epochs_loss_values'],axes[0][0],\"training loss\",\"red\",\"x_batches\",\"loss\")\n",
    "display_axis(log_data['validation']['val_epochs_loss_values'],axes[0][1],\"validation loss\",\"blue\",\"batches\",\"loss\")\n",
    "display_axis(log_data['training']['train_batch_loss'],axes[1][0],\"training batch loss\",\"red\",\"batches\",\"loss\")\n",
    "display_axis(log_data['validation']['val_batch_loss'],axes[1][1],\"validation batch loss\",\"blue\",\"batches\",\"loss\")\n",
    "display_axis(log_data['training']['train_f1score'],axes[2][0],\"training f1score\",\"red\",\"batches\",\"f1score\")\n",
    "display_axis(log_data['validation']['val_f1score'],axes[2][1],\"validation f1score\",\"blue\",\"batches\",\"f1score\")\n",
    "display_axis(log_data['training']['train_accuracy'],axes[3][0],\"training acuu\",\"red\",\"batches\",\"accuracy\")\n",
    "display_axis(log_data['validation']['val_accuracy'],axes[3][1],\"validation acuu\",\"blue\",\"batches\",\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def merge_itrs(*itrs):\n",
    "    for itr in itrs:\n",
    "        for v in itr:\n",
    "            yield v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plR0dVaJcz06",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for image,test_label in merge_itrs(test_loader,val_loader):\n",
    "  model.eval()\n",
    "  test_metrics.reset()\n",
    "  with torch.no_grad():\n",
    "    image = image.to(device)\n",
    "    test_label = test_label.to(device).squeeze()\n",
    "    output = model(image)\n",
    "    test_metrics.update(output,test_label)\n",
    "    confmat.update(output,test_label)\n",
    "  #computing for overall batches\n",
    "  computed_test_metrics = test_metrics.compute()\n",
    "  for metric,value in computed_test_metrics.items():\n",
    "    log_data['testing'][metric].append(value.item())\n",
    "\n",
    "cm = confmat.compute()\n",
    "fig,ax = plot_confusion_matrix(cm,labels = class_encode.keys(),cmap = 'Blues')\n",
    "plt.show()\n",
    "confmat.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TAJjzGQ_h8h",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Saving the metric values\n",
    "with open(metrics_saving_path,'wb') as file:\n",
    "    pickle.dump({'log_data':log_data,'cm':cm},file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8660165,
     "sourceId": 13625885,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 491381,
     "modelInstanceId": 475485,
     "sourceId": 630949,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "3.13.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
